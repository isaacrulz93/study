{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.2 신경망에서의 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금 우리가 이제 함수에서의 기울기에 대해 알아봤잖아, 이젠 정말 우리가 '신경망'을 다룰때의 기울기에 대해서 알아보자.<br>\n",
    "신경망에서 기울기란, 가중치에 대한 손실함수의 기울기를 의미하는거야.<br>\n",
    "\n",
    "예를들어, $2 \\times 3$ 의 가중치 $W$, 손실함수가 $L$인 경우를 봤을때, 이때의 경사는 \n",
    "<font size = '4em'>${{\\partial L}\\over{\\partial W}}$</font>로 나타낼 수 있어.<br>\n",
    "수식으로 쓰면 다음과 같아<br><br>\n",
    "\n",
    "<center>$W = \\begin{pmatrix} w_{11} & w_{12} & w_{13}  \\\\ w_{21} & w_{22} & w_{23} \\end{pmatrix}$</center>\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "${{\\partial L}\\over{\\partial W}} = \\begin{pmatrix} {{\\partial L}\\over{\\partial w_{11}}} & {{\\partial L}\\over{\\partial w_{12}}} & {{\\partial L}\\over{\\partial w_{13}}}  \\\\ {{\\partial L}\\over{\\partial w_{21}}} & {{\\partial L}\\over{\\partial w_{22}}} & {{\\partial L}\\over{\\partial W_{23}}} \\end{pmatrix}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 ${{\\partial L}\\over{\\partial w_{11}}}$은 $w_{11}$을 조금 변경했을때 손실함수 $L$이 얼마나 변했냐를 나타내주는거야<br>\n",
    "둘이 형태가 같다는거에 주목해주면 좋을것 같아. 이게 실제로 신경망에서 어떻게 적용되나 한번 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 함수들\n",
    "def softmax(a):\n",
    "    c = np.max(a)\n",
    "    exp_a = np.exp(a-c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    return exp_a / sum_exp_a\n",
    "\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-4\n",
    "    return -np.dot(t,np.log(y + delta))\n",
    "\n",
    "def numerical_gradient_1d(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "       \n",
    "    return grad\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim ==1:\n",
    "        return numerical_gradient_1d(f,X)\n",
    "    \n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = numerical_gradient_1d(f,x)\n",
    "            \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[1 2 3]\n",
      "1\n",
      "[4 5 6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[1,2,3],[4,5,6]])\n",
    "for idx, x in enumerate(X):\n",
    "    print(idx)\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        np.random.seed(1) #seed를 쓰면, random값이 고정된다\n",
    "        self.W = np.random.randn(2,3)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "    \n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        return loss\n",
    "SimpleNet = SimpleNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 6 2 9 8]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "print(np.random.randint(1,10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleNet을 설명하자면, 형상이 $2 \\times 3$인 가중치 $W$를 변수로 갖고,<br>\n",
    "predict(x)와 loss(x,t)라는 메서드를 가져.<br>\n",
    "x는 input, t는 정답레이블이야.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그래서 만든 가중치가 어떻게 생겼는데? \n",
      " [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "\n",
      "계산기엔 이 숫자를 집어 넣을거야 : [0.6 0.9]\n",
      "\n",
      "그 가중치로 예측한 값이 이거야 : [ 0.00893546  0.41181302 -2.38828788]\n",
      "이때 예측한 값중에 가장 큰 인덱스가 얘야 : 1\n",
      "\n",
      "정답은 이거야 : [0 0 1]\n",
      "이럴떄의 손실값은 : 3.344918677423258 가 나와\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet\n",
    "print('그래서 만든 가중치가 어떻게 생겼는데? \\n',net.W)\n",
    "print('')\n",
    "x = np.array([0.6,0.9])\n",
    "print('계산기엔 이 숫자를 집어 넣을거야 :',x)\n",
    "p = net.predict(x)\n",
    "print('')\n",
    "print('그 가중치로 예측한 값이 이거야 :',p)\n",
    "print('이때 예측한 값중에 가장 큰 인덱스가 얘야 :',np.argmax(p))\n",
    "print('')\n",
    "\n",
    "t = np.array([0,0,1])\n",
    "print('정답은 이거야 :',t)\n",
    "\n",
    "net.loss(x,t)\n",
    "print('이럴떄의 손실값은 :',net.loss(x,t),'가 나와')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "생성한 가중치 W : \n",
      " [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "\n",
      "예측한 p값 : [ 0.00893546  0.41181302 -2.38828788]\n",
      "최댓값의 인덱스 : 1\n",
      "\n",
      "이 네트워크의 손실값 : 3.344918677423258\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet()\n",
    "print('생성한 가중치 W : \\n',net.W)\n",
    "print('')\n",
    "\n",
    "x = np.array([0.6,0.9])\n",
    "p = net.predict(x)\n",
    "print('예측한 p값 :',p)\n",
    "print('최댓값의 인덱스 :',np.argmax(p))\n",
    "print('')\n",
    "t = np.array([0,0,1])\n",
    "print('이 네트워크의 손실값 :',net.loss(x,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 기울기를 구해보자. 앞에서 썻던 numerical_gradient(f,x)를 쓸거야.<br>\n",
    "f(W)의 W는 지금 그냥 보여줄라고 임의로 만든 더미(가짜데이터)니까, 값에 너무 의미부여하지말고 그냥 어떻게 진행되나만 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.23126277  0.34599771 -0.57726048]\n",
      " [ 0.34689416  0.51899657 -0.86589073]]\n"
     ]
    }
   ],
   "source": [
    "def f(W):\n",
    "    return net.loss(x,t)\n",
    "\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를들여 $dW_{11}$을 보면, 0.2이라는 뜻은 $w_{11}$을 h만큼 늘리면 손실함수가 0.2h 만큼 증가한다는 뜻이야.<br>\n",
    "$dW_{13}$을 보면 -0.6 정도 되니까, $dW_{13}$를 h만큼 늘리면 손실함수가 0.6만큼 감소한다는 뜻이지.<br>\n",
    "<b>손실함수가 줄어들수록</b> 좋은거라고 했지?\n",
    "이럴땐, $dW_{11}$ 는 감소시키고, $dW_{13}$는 증가시키면 손실함수를 줄이면서 갱신할수있어!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 학습 알고리즘 구현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "제일처음에 우리가 퍼셉트론을 통해 논리게이트를 구현했었잖아.<br>\n",
    "이번엔 여태까지 배운 '학습'의 과정을 통해 논리게이트를 구현해볼꺼야!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 논리게이트 학습하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용할 함수들이야! 다 해본거야 겁먹지마!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "lr = 0.1\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "def sum_squared_error(pred_y, true_y):\n",
    "    return 0.5 * np.sum((pred_y - true_y)**2)\n",
    "\n",
    "def cross_entropy_error(pred_y, true_y):\n",
    "    \n",
    "    if true_y.ndim ==1:\n",
    "        true_y = true_y.reshape(1,-1)\n",
    "        pred_y = pred_y.reshape(1,-1)\n",
    "    \n",
    "    delta = 1e-7\n",
    "    return -np.sum(true_y * np.log(pred_y + delta))\n",
    "\n",
    "def cross_entropy_error_batch(pred_y, true_y):\n",
    "    if true_y.ndim ==1:\n",
    "        true_y = true_y.reshape(1,-1)\n",
    "        pred_y = pred_y.reshape(1,-1)\n",
    "        \n",
    "    delta = 1e-7\n",
    "    batch_size = pred_y.shape[0]\n",
    "    return -np.sum(true_y * np.log(pred_y + delta)) / batch_size\n",
    "\n",
    "def cross_entropy_error_for_bin(pred_y,true_y):\n",
    "    return 0.5 * np.sum((-true_y * np.log(pred_y) - (1 - true_y) * np.log(1-pred_y)))\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    return exp_a / sum_exp_a\n",
    "\n",
    "def differential(f,x):\n",
    "    h = 1e-5\n",
    "    diff = np.zeros_like(x)\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        temp_val = x[i]\n",
    "        \n",
    "        x[i] = temp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        x[i] = temp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        diff[i] = (fxh1 - fxh2) / (2*h)\n",
    "        x[i] = temp_val\n",
    "        \n",
    "    return diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.18495159, 0.93670217])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 신경망 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogicGate():\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         def weight_init():      #초기 가중치 값을 만들어줄거야!\n",
    "#             np.random.seed(1)  # 시드를 고정시켜주면 랜덤값이 그 시드에 맞게 생성돼. 돌릴때마다 가중치값이 변하는걸 방지해줘.\n",
    "#             weights = np.random.randn(2) #표준정규분포의 난수 2개 생성\n",
    "#             bias = np.random.rand(1) # 0~1사이의 균일분포 난수 하나 생성\n",
    "            \n",
    "#             return weights,bias\n",
    "        \n",
    "#         self.weights, self.bias = weight_init()  #이 클래스의 가중치와 편향을 weight_init에서 생성된걸 쓸거야\n",
    "        \n",
    "        \n",
    "    def __init__(self):\n",
    "        np.random.seed(1)  # 시드를 고정시켜주면 랜덤값이 그 시드에 맞게 생성돼. 돌릴때마다 가중치값이 변하는걸 방지해줘.\n",
    "        self.weights = np.random.randn(2) #표준정규분포의 난수 2개 생성\n",
    "        self.bias = np.random.rand(1) # 0~1사이의 균일분포 난수 하나 생성 \n",
    "\n",
    "        \n",
    "    def predict(self,x):\n",
    "        W = self.weights.reshape(-1,1)\n",
    "        b = self.bias\n",
    "            \n",
    "        pred_y = sigmoid(np.dot(x,W) + b)\n",
    "        return pred_y\n",
    "        \n",
    "    def loss(self,x,true_y):\n",
    "        pred_y = self.predict(x)\n",
    "        return cross_entropy_error_for_bin(pred_y,true_y)\n",
    "        \n",
    "    def get_gradient(self,x,t):\n",
    "        def loss_grad(grad):\n",
    "            return self.loss(x,t)\n",
    "            \n",
    "        grad_W = differential(loss_grad, self.weights)\n",
    "        grad_b = differential(loss_grad, self.bias)\n",
    "            \n",
    "        return grad_W, grad_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1.1 AND 게이트 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs : 100, Loss : 0.6886489498071491, Weights : [1.56426876 0.79168393], Bias : [-2.14871589]\n",
      "Epochs : 200, Loss : 0.4946368603064415, Weights : [2.01360719 1.71241131], Bias : [-3.07894028]\n",
      "Epochs : 300, Loss : 0.3920165980757418, Weights : [2.42841657 2.29753793], Bias : [-3.79103207]\n",
      "Epochs : 400, Loss : 0.3257214374791936, Weights : [2.794852   2.73235738], Bias : [-4.37257095]\n",
      "Epochs : 500, Loss : 0.27863601334755067, Weights : [3.11636193 3.08408364], Bias : [-4.86571237]\n",
      "Epochs : 600, Loss : 0.24328504683831248, Weights : [3.40015395 3.38235762], Bias : [-5.29433736]\n",
      "Epochs : 700, Loss : 0.21572536552468008, Weights : [3.65300561 3.64264217], Bias : [-5.67349792]\n",
      "Epochs : 800, Loss : 0.19363244428365756, Weights : [3.88044124 3.87412053], Bias : [-6.01340133]\n",
      "Epochs : 900, Loss : 0.1755321312790001, Weights : [4.08680123 4.08279091], Bias : [-6.32133891]\n",
      "Epochs : 1000, Loss : 0.1604392693330146, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n"
     ]
    }
   ],
   "source": [
    "#AND게이트 정의\n",
    "AND = LogicGate()\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]]) #AND게이트의 input\n",
    "Y1 = np.array([[0],[0],[0],[1]])        #AND게이트의 label, output, 정답\n",
    "\n",
    "train_loss_list1 = list() #빈 리스트 생성(기록을 보기 위함)\n",
    "\n",
    "for i in range(epochs):\n",
    "    grad_W,grad_b = AND.get_gradient(X, Y1)\n",
    "    \n",
    "    AND.weights -= lr*grad_W\n",
    "    AND.bias -= lr*grad_b\n",
    "    \n",
    "    loss = AND.loss(X,Y1)\n",
    "    train_loss_list1.append(loss)\n",
    "    \n",
    "    if i%100 ==99:\n",
    "        print('Epochs : {}, Loss : {}, Weights : {}, Bias : {}'.format(i+1, loss, AND.weights, AND.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00135483],\n",
       "       [0.08867878],\n",
       "       [0.08889176],\n",
       "       [0.87496677]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AND.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오... AND 게이트를 학습시킨걸 볼 수 있어."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1.2 OR게이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs : 100, Cost : 0.49580923848195635, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 200, Cost : 0.3398674231515118, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 300, Cost : 0.2573360986187996, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 400, Cost : 0.20630142190075948, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 500, Cost : 0.1716549922113493, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 600, Cost : 0.1466501884550824, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 700, Cost : 0.12779768649454676, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 800, Cost : 0.11310517185413338, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 900, Cost : 0.10135180918376233, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 1000, Cost : 0.09174843008614178, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n"
     ]
    }
   ],
   "source": [
    "OR = LogicGate()\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y2 = np.array([[0],[1],[1],[1]])\n",
    "\n",
    "train_loss_list2 = list()\n",
    "\n",
    "for i in range(epochs):\n",
    "    grad_W, grad_b = OR.get_gradient(X,Y2)\n",
    "    \n",
    "    OR.weights -= lr * grad_W\n",
    "    OR.bias -= lr * grad_b\n",
    "    \n",
    "    loss = OR.loss(X,Y2)\n",
    "    train_loss_list2.append(loss)\n",
    "    \n",
    "    if i%100 ==99:\n",
    "        print('Epochs : {}, Cost : {}, Weights : {}, Bias : {}'.format(i+1, loss, AND.weights, AND.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.09855987],\n",
       "       [0.9600543 ],\n",
       "       [0.96195283],\n",
       "       [0.9998201 ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OR.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1.3 NAND게이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs : 100, Cost : 0.7911738653769252, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 200, Cost : 0.5430490957885361, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 300, Cost : 0.4212591302740578, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 400, Cost : 0.3456117101527486, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 500, Cost : 0.2931298605179329, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 600, Cost : 0.2543396786002071, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 700, Cost : 0.22443918596775067, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 800, Cost : 0.20067626330853877, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 900, Cost : 0.18134125517637367, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 1000, Cost : 0.1653094408173465, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n"
     ]
    }
   ],
   "source": [
    "NAND = LogicGate()\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y3 = np.array([[1],[1],[1],[0]])\n",
    "train_loss_list3 = list()\n",
    "\n",
    "for i in range(epochs):\n",
    "    grad_W, grad_b = NAND.get_gradient(X,Y3)\n",
    "    \n",
    "    NAND.weights -= lr*grad_W\n",
    "    NAND.bias -= lr*grad_b\n",
    "    \n",
    "    loss = NAND.loss(X,Y3)\n",
    "    train_loss_list3.append(loss)\n",
    "    \n",
    "    if i%100 ==99:\n",
    "        print('Epochs : {}, Cost : {}, Weights : {}, Bias : {}'.format(i+1, loss, AND.weights, AND.bias))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99851256],\n",
       "       [0.90861957],\n",
       "       [0.90879523],\n",
       "       [0.12861037]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NAND.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1.3 XOR게이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs : 100, Cost : 1.4026852245456056, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 200, Cost : 1.3879445622848308, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 300, Cost : 1.386492030048381, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 400, Cost : 1.3863236205351948, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 500, Cost : 1.3862994743646844, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 600, Cost : 1.3862953430687464, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 700, Cost : 1.3862945581495083, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 800, Cost : 1.38629440139037, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 900, Cost : 1.3862943694120307, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 1000, Cost : 1.386294362832352, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n"
     ]
    }
   ],
   "source": [
    "XOR = LogicGate()\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y4 = np.array([[0],[1],[1],[0]])\n",
    "train_loss_list4 = list()\n",
    "for i in range(epochs):\n",
    "    grad_W, grad_b = XOR.get_gradient(X,Y4)\n",
    "    \n",
    "    XOR.weights -= lr*grad_W\n",
    "    XOR.bias -= lr*grad_b\n",
    "    \n",
    "    loss = XOR.loss(X,Y4)\n",
    "    train_loss_list4.append(loss)\n",
    "    \n",
    "    if i%100 ==99:\n",
    "        print('Epochs : {}, Cost : {}, Weights : {}, Bias : {}'.format(i+1, loss, AND.weights, AND.bias))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49996646],\n",
       "       [0.49999372],\n",
       "       [0.49999575],\n",
       "       [0.50002302]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XOR.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR는 학습이 잘 되지 않았읍니다....\n",
    "\n",
    "왜?\n",
    "\n",
    "1층신경망이라서!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 XOR 2층 신경망 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자 우리가 처음에 퍼셉트론 배울때, XOR문제를 어떻게 해결했나 다시 잘 생각해보자!"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWUAAAErCAYAAAAG15KnAAAgAElEQVR4Ae2dO48jN7qG67fsHxLQgOPzTwaCzwTOnC2gTBP0BJsNJjFwErUD7wAbDNDAZF64F3bD6URWsLvmAW9VJItkXUVRpaeBhi5VxcvLrx5++oqXRvCHAiiAAihQjQJNNSWhICiAAiiAAgIoYwQogAIoUJECQLmixqAoKIACKACUsQEUQAEUqEgBoFxRY1AUFEABFADK2AAKoAAKVKRAFspfv34VX758EZ8/f+YfDbABbAAbKGADWShLIEsw//HHH+J8Plfx/89//rOKctSiB+Wowy5pB9phLRvIQll6yGtltFY6QBnjX8uWSAdbqtEGgHIlvwBqNA7KBLSwgfI2AJSBcnW/hgBBeRCgeT2aA2WgDJSxAWygIhsAyhU1Bt5KPd4KbUFbXMsGgDJQxkvCBrCBimwAKFfUGNfqmckXrxAbqMcGNgvll4870ex34vjLNLH1dY1o9o1ovj+KF6CNF4UNYAMFbQAou2L/chS7fSN2H1/qNMKfj2LXNKJp9uLklrvG97dU1hr1o0x13oMF2uVqUH7+eBCHw0E8/vg6Sfyxk0dmecqf9spD3n+a4F0//SmaRoj9k3PNz/8Vu0aI3bt/67qZz/I8+d9+Lxs4ONY73hrBSeybnTj+7OTTHhv47mkvGgnzh3U9/5d3O52u6igasXsX6cxk3ivny0/tgfYeaxecN4k9peyuPJR/exKPCsbP4ulxA1A+n8XLuz9F8/DfNtRxeiNE8+Y/psH/LY4PLoj/I/YuxBWU/+w8XwNpD/Lnszi9aUTz5jTRiF7E8UHD+CQBuiocT2LvpSc7jRiYdRmiwAYKE9sTGJcC4zXzKQ/l9kZ8XR3Kp6OJBe/34hiJKXfH5XluCOAk9jKGHP6Pjik7oDWQbT3a8LMCrAPtEMrhcamXCgVEvGQbInBgbb3XEOrqew+iAzf4hLStASc7DuWpu3rbvDXIbyIc09qtLTuvtt15XdcWNgNlP1xhIds96NNAtp9fxPH7yIO8OeELc7Nab/noeclncVbhDccTDj3rMVBOQi0EtoZcCGR500yGcq8zSKetb8qMR5zqVM4WypEOBwjiRd+pDWwEyn3IRiF97H7++8dNT7cAyuezDlM0jQ9gBWUntKEAJkFtvwuhrD674Y5hoFrgHmWII+EN23Omjiax1+XS1nWSceuYNyy1HQL6up6G7iRIEx1u0wa2BeUUdM2oil54IhwydwEoWw/ag6HrPRsI24eA6rWNR2ujUmGBBGz1jTfscVq4euUY5YkMp63DK43/sNNLW6dBXPk2IQHcy7bbRqBswhVDUHaORw1tAZQtfJeFL2Rs+s/eCIthoA6DcziNlOENpT0GuPqcWFgl2g4e0FPl4nu026YNbATK08MXUYOeDeX1HvRZuHsebS6mbEdmPBxFLsSQhrIZoZEIPVgvPZ62udZ50BjVdTCmnAp7bPOmi2pER0QM3djARqB8Fn6MuP+gzz8ub/YXcTwG43ZnQtkfAhcOkQuGxJlwRes1hjHlswP49kbNeJoK2PZBWdprTUNZapFIfyBtC2yvA2nL3AE1nbfOt5k7/jqSF8DrdEeL29TiClDWQ+HkxBH3/8PzOAHTk0eMt6yGte3FSQHWjrbQaftD4tYZfaE8217IQYO1HascxI37k0f8h4M6TX9CShRsZtiaF6tVIO3GC6vrzOQONYFEvu95thGYD6ZtgSpnGDr/ibTbTgiQ4hFiA1kbuAKUx8E31cunobws3VR+9XyfGXK20MjHerxztFBp90C99baifnNshWu03QDlhUAra0j5+O/kshhvODWMbnJ6rpY2bYCc9YoWaezqzfvN6AyUMebNGDOAw0Pfgg0AZaAMlLEBbKAiGwDKFTXGFnp56oC3ig0sswGgDJTxkrABbKAiGwDKFTUGHsYyDwP90G8LNgCUgXIBL8mOGsmtj1ELUOz4azshp5ZyUY4tAHdMHa4A5WfxwZs48kE8TwDjzY5TtkPEEtOZxzTW5HPMRBJ3sogcN+xNNrHTtHuz6ixIHTipOpgp0W19uokjqQkisTxH1aXNY+Vp2G26puyxxZ7cuk6wz1H1Ir0CjsDtdmLFofz646NwZ+/Jz4fDeDDfIpTVBAoJYwXJlQGTu8Flfg87sXM6gj4gpWe4E7uHENZmsoq83m7z5IJKvXeAbTqAHphn1vmSmp3eOOW2nVIEzKoMjLMGoLl77ALHikO550mo7aEexdNv43q2OqFsf/IOeI0zAdXTbKwhKCjrhYosLHtQNue8yFcPQBrK+3dHsbPAykHZwi2SRgt1W26Vjj/d204Ht+Vs6zxZM9MWtswyT5VG2Ok49pbKI+x4bPl5BdQXtIHrQ/n5w817yv11KU7i5G6kahswdfPb473X/4j/+4sQf4v8f3hvNmXtXRPARsJJ5mtgGUJZlV0ec4Gr0jRQfpKvxrN0z4kBS+bjwjC10JFM37teg7QHZHneZM1k/d30jMdvvf2IXv32sxq66djveG07zIiWHFtuH1eG8vR9+qr0lBU4/J/EUeOcBZgFjWwh6cDUh7IFr8zDfe9/bq9x0vGhasoY1s8Db78eFobxZUETaY4FgSnLUe24nQsZ5cA7DPRoO48tI+fhcUds4KpQfv54EIfHJ/EaKVjK2KuEsiq/voHlimlRj0+eE0JrQr1TemS/t1B2gNsCVubtQrYXfnAgbdNxz48BN6yf+pzrrDQQs0t3hmmO1mxEexhd7K+IvpYmDS8k0+9c+tdxDprMt4GrQXnqAz7byPVC2TZC5kaeDJiVwhd249Q3J+FBWZWni4OrJTjb8IMDZRUO2Inj07F7aJiCcnu9hX69UJZa+OEW24b2VWvQi4mP7hhsOrza+5fXYVu4DpRVHHn8wz23IeuHcsYjngzl4QZ0tem9l/lZSBovd+8MifMArUAjPVcLURfKZw3zN/sslBXkPK8yFxrQacryzQ1fqPBH4peJPrYXqfCFPX7KAjZf/p7e2bQWtiVp302oozyU1WiLgzcsbopx1wdlCS8/Zpn0wK4JZfVT3R36FgOO6xn6UFahF3d4XegpG687DN30QW3gpM63HYAuS9QjHdAsmr4qmw0j6Xp4IQov7wwsB/KeYrecm9GZDsfrcIpDWYct/F1HDofxXnN9UJbbP+383TecccHqZlQ3dypMcEFjlflaT9mGMBo9NCzlKXbfB1A2IxoaWzcDPnfXkRDIqu4hvOUNaK71IGw0ar8bqVm/A9SA9yBsy+qMQHHLrd/7Hat98NmWB3B44KCTudx9WxzKSxuzRigvrdPWr1egdzqH1eqrwG297XVvkouVGbgD9wEbAMoDAq0GkDvPR3vgNqSwFKAmJNHGv5em515vPO1LdCJ3bgPcS66dpd8DZW4UPBdsABuoyAaAckWNgSeR9h7QBm3uxQaAMlDGS8IGsIGKbAAoV9QY9+IJUE+8XmwgbQNAGSjjJWED2EBFNgCUK2qMqr0HO274BkYl2JEe3lhl2hnw3ogNlIeymdF3aHcfGT9xRELrFscpt5Bo9ASSohMSLExN3u4Ej7BcyYWB1OSLcHJF+ueX17nY/FeGuZo0YuokJ3+49bL5y3OKan0jN73Vh9eRNly4XctD+fnJW9B+6sJEtwflk9h7QMpMKV698XVeLbCC2XUKys5aFRrSIXz1mODpcDNjiR+O4iRnPHoaLLwZZD2cctsZgm09rY5BfYHQQt2trrxe9FdHeSiHDXpPO4+YukfXawh1WeOz8lJdyPqADaHsLw5vbuBeGvp762V3ILQTOtz8nHMnQHlK2hq0fr1c+BbTeo32Io2Lws61i5rfXx3Kak3lG9+jT0HEg05i5xF106UB0jeUZUt39stlVmYzXqY67nqc3o4dw0D10k/AW9bJO28keLxrMmlrzYJfBE4eXjrO932tTSfEOYDxyjZwJSi7O1qP3zRV3khVhi8UNEauwTAImPXgoIDkQdcAMgFldX4wdTnvadoO5ij2ZqGjGOzmgXFc2jK/bPoF9Y7Vne/Ws+d70fJKUO4aSsWUPz6P7p2rhLLqWTVEUg+dlEGpGGf8odQlDC4GVBdgGsL+6nVdOEK2kalTAHavrAp6+cXi3Ty9a4c8khFp691cMh2iSiNzfKgMHB99b05qW3RN6np1KJ/P2mv+8NyBOte49ULZlj8FsjkP+AqGLyS8vBCMrk8M7F77jADnxaA8ppNT5evHub06AIgkINDJ3tflXoHyJW7IHghSoL5wQyfKYUdSKFi2XrAuoz1mb8Y8UG1HMzd8oa+PdQb2oePuXSptm/dLFij58l9Y/0vYFmlm29va7S2/Fobyq3j66G+UevsP+iTMfE9MeZeO1xl+LmcwGlxtSEJBuvsp70M5sY1VZliZV68gbbeOWTAm0s+nPbaTi3c0btl4T8dUmw0UhvJZaAi7O4/c/oM+BR1nIkO7O4fyaow36B1vRLHZZgqWXdy4BbR9QNZ6yvLmNLCzu4sYr0wB0jvP7rbSAV4atjrPeVDY1yVS70gIQl+XSdtcE+4eEnr5xXcPx4vdvBdbAuDFoby0UvXHlLfoeQQe92rwGevxztH0UmWeUxauWXrf39P1QHk1wGz9xjMevxOWWXSjWA8+8MAXpWna0nro7q+CNdIlja3beB31A8pAmZ+c2AA2UJENAOWKGgNPrA5PhXagHa5pA0AZKOMlYQPYQEU2AJQraoxr9s7kjXeIDdRhA0AZKOMlYQPYQEU2AJQraoyqPRU7WmKt0RcX1N2Ovig2FvyCdanaJqj3RTqzq0JZL3B/EGPXvZAGyjjl2E+s+KQPPXnCn7AhJ3jYSRZ6skc3sSQ+3fks9CLy/qzF0bC4EMzDsseGv7l1HV1eQHMR0KB/7L6Nf3dFKMuFiB7F4yNQXmywZqry7iFYgU4C8WEnds4MPRdUCmztOGED9p4nrL+3IB9f1i49dh6J33zjteT6e9LqalCW060/PL+Kp1uHcmSacHJ7ogt5Yern+puTWlfYg6eC8lEc33SwTkPZesT+9ObsVGXrBbdTyOPetCpfD/YDoBmZtr5Z0x2H3/EM5Hmh9rknoFDX5TZ2HSg/fxAHtYbyBqBs15BwoWNg+DLiJn/+XyH+9pfI///8V/w64np5E7SgDfO1n+Wr8Yjbc+1aFa2nLI1Jw80NBaSBGgHh00mcImVOp5Ey4PFpawikp1RPzztVJr4HuGVs4ApQ1mGLp99kBbcBZe0Zdx6mC77LG7IEks3bfW9WfZOdhfLmtRfrlk2+t7DW5ezDsH9OZ5jqmNsZRYAs050DxrFpD6YvOyQnfHP59uj0IS+0mGMDxaEswxaPP76ahwkbgbLxMHXoQIIx/jN+TgMNXmO9YQXEAKrtMf299IAl7GyIow/c4Hq7apznTQc3WrtiW7rOc6Cs6j0ibXYeCdoj0TEO2hHXVfOAszCU3b353OU7D+Lw6K+znDKiakdfWADK1xzEAuNfGr5QwGtjumYkhc3flsl6q29Oo6Dshi/64E5AIBZbN3WdDWWrVSrt1Pf2OvkqNSjZSbp5874a0KV4UuP3haEc3tBb8ZRlvXTooDcC4qI3ZucBt8alQGW8VgfKdljbPucpq2ttKES31RSgpgCeTkNqlt/fz9arn7a+1nr99rzwNZ13aIt8DrXj83VsAiivCM3iAIhA1G6jpLxdF8oqFLETstOwIPNBpwHfG6sczcOM1LAeudIwDcmsLrH05XfZtE1ZvXNiN1AYjomdw3fAty4bAMorQ9kCr4ShK6hGHrS133tQtruFBFB2Qx+RtGQ9fHhbAzZgdK8PIKlg7B6X74Nz4sMHB9JWIHcmvZg8etoTuiB8sOL9XeKelnlcGcr2Bh//Wm1M2Xiix5/H16VUIy/PR3vBbqx5eZpSp7Ee7xxNL1XmOWXhmnXs5T50BMor9KTKk4x5gSukXY8xa8j1whtz66i82IjnPDc95zrroa/fidwHFOqxufvUGyg7NzPGeJ83Ae1Ou9dkA0AZKBN3xAawgYpsAChX1Bg19daUBe8RG7iODQBloIyXhA1gAxXZAFCuqDHwTK7jmaA7utdkA0C5Iii/fDqJMSvL1WRAlAWgYQPr2sAVoBxZ/0It4zmuYvWOUx5X/pQBn46NaL4/AuWKOslUW/H9MltHv7x+5aH825N4PHwQzzNvPqCcb1AMHn2wgdu2AaA8s3PwDP+Xo9jtG7H/9CKO3zei2ev//SdrHCexN9+pY8dT92Dl0749314nX+21yoPed8tivnzciWa/E8dfTNq5vFXaO3H8pMvXyztXdzOV2ZuAEfsulwbHunZGC7QYaQNXgrK7bOc0r7lKT9mA0YPl+UW8KHAaINvQhDl39/HFM9JU+GIslKN5W+DbvM1nC3yvY4kYTG8hoWAtjaHrOW47ZV6xhfE2cAUov4pXBwBy0fuxaynLhq0ayq4HbOvYA6Hxpi0ozXmLoZzJu+0AEh1C8oZRnnG3lKecTt5b9MfWk1evk01qik7oNGAD5aEcFkjFmB+F3h5quDepGcot/Jw66nBDF9JoQxQrQzmW9znsEKZC2SwYpEEs177owihAZ9hW0QiN5tgAUHYAOkdAdU0GdhbKQyGDpZ7yZaDs7PMnQxfhsptraEcaeI7YgGcD14ey3Nl6wmiMW/OUe95qwgDHQlnHmPsP+i4G5avsqIKHNdtBSNgX6d2OTRWG8qt4+vHZ6RXMmOVbH6ec8ZTVmsFyRIYbrvjlKPaxB33OKAt7E/mjLewojpJQnrcbtS0/r7cDA9qqjrYqDOWzeP3xURwOzuiLCUCWRnNznrLyXCxMu9hy37P1z+nCHe4wu704qThxeSjzgK+OGxZwbr8dikN5qVFVCeVN/2SUu4N0IzCWth/Xbx8qtPGyNgbKmwbqMuO4jx1VlmkEgNBvbRsAykDZifFzg619g5EeNjXVBoAyUAbK2AA2UJENAOWKGmNqj8r5eGHYwPZsACgDZbwkbAAbqMgGgHJFjYHXsz2vhzalTafaAFAGynhJ2AA2UJENAOWKGmNqj8r5eGHYwPZsACgDZbwkbAAbqMgGslD+8uVLdY3FjL7teQZ4e7QpNtDZQBbKX79+FRLMnz9/5h8NsAFsABsoYANZKAv+UAAFUAAFiioAlIvKTWYogAIokFcAKOf14SgKoAAKFFUAKBeVm8xQAAVQIK8AUM7rw1EUQAEUKKoAUC4qN5mhAAqgQF4BoJzXh6MogAIoUFQBoFxUbjJDARRAgbwCQDmvD0dRAAVQoKgCQLmo3GSGAiiAAnkFgHJeH46iAAqgQFEFgHJRuckMBVAABfIKAOW8PhxFARRAgaIKZKHMKnGsjscKgdgANlDWBrJQlst2SjD/8ccf1ayrzHrK3bqrrEGLFtjA9mwgC2XZQ9bW6EB5e0ZYm41RHmzsmjYAlCvaBuaahkDegAgbqMMGgDJQru7XEHCoAw60w3XaASgDZaCMDWADFdkAUK6oMfBMruOZoDu612QDQBko4yVhA9hARTYAlCtqjJp6a8qC94gNXMcGgDJQxkvCBrCBimwAKHuNcRL7fSOa4+liRnp604imacTu3cvF8oh7OC/i+KDz3j8V8AB+PopdI/Pbi5Onsc77ejoUqHukvvE2oSzo0reBq0H5+eNBHA4H8fjj6yQ4XXTyyC9HsZNQ/v4oXmbcWC8f9+L4S19ka3gv73aieeMC34Dywc9PA6uDmQWYhLkPuZPYq+80bNVxL/2uLDKNaEfwj71ovmv0/1/9cthyp19fxPGv5trvGrH/R5dfe83TXjRB/dpjZ13/Ip3EjPbsyhmpF+lNum/RcrwNlYfyb0/iUcH4WTw9VgblRTea9LJ3aSgrz3Enjj+HjaPB2oIpPE9CzfE2FdhbyMlr3TR1Wj34Bmm0N8i/jmL33U4c/6XLdHrfiOa922mEZfU/e+cHabV5GPD2ymS1TpXNHucV+N2ZDZSHcivw66pQfvm4E83xKI7fN6KR3m4ISOUFS+/ThCjUOY3Yf9KgUdeb73Yf/dCCTvskTsd42t339ng/BKK83YQX64I2PC/8fPagHUL5LNy0NBi1NxqDogfV81mozw6kO7D6MFbfhxBWnxNQz4I3Xb5s/q0dRcrGMUB+wzawLSjvA8i6YQgbmth3YYHz+UW8eOGGFwX1KJRzaSsDyHnKQ+Ax3vI7GYd1Pd/YdeZcFRceA2X3fBdgOvSw+8F0QDKM8X4v9qkwRGjkKuxhtZRp7cT+/U40sRCI15G4ZdDvex1PmBefgewd2cC2oOxCWHnETjjBQNl6xnEvLANlN+3W63YBk4NyCozO9cqbDOO+GsptaEMZpvtdCGWdjxe3TgJRQ1nHgU9i/535FTEFygbALz/shIS7fI1CWbZF0wi/Hl3d+959dyzeThxHl+3awLag7I2a0GGKFsIKpA6koz1vBspu2lOhrMCYhpK6waJQjsHM9Z4NhN2HfW282RitStf1vq0xd1CWMNVw7r4bvOmlp6ygfBJ76x2339k87KsuZyyEIvNRUHbi5oN5R9vO5sUr+t22DQBl7wa/EJQHPMWzPb4ofOF60I5RDnjKu/d7sWsf7kmPOTGKwtPpLM4mfCFDFvZBIZ6yo3uoF58JwYy0gW1BuRdicDzja3rKAyMQ3J/vYXw1/Jx90Bd9oBbztjU8eg/2wod3yoiMNx564JEHe+GDw9ZjS3YMphxy7HbiIWibxkiD5nw6hlu3gW1B2RlxoUZE5CAdvcmXesrdg8bQMFzwesd6wAogGoBWQboFpDzXDU24oY3u5uyB3dbde1hnRl+0XnN3vd8R2O91qKN9UBgFuj43WXcH+ql4s6eVLTeveJ0btoErQFkPhZMTR9z/D8/2Zs+/piaP9IfE2ZEBJr2sp+wPk9ND6rphbXZIXAuIaEz5LM6f9mY4XmwCSgBbB0ihl6hjrB1sFVTbuLFbrxDKZ3GOxaZ74O80ViEHO3lEPezrjrX1VdfHYuI63GEnn7SA9m6YWL27PPLA7s5ry+KlzXF02Z4NXAHKy0TMQ3n8xIerGHPg9ZYsw3z4ae877DjGlj3ppUu4ZjqLselz3rL7Cf3q0w8ol/a8Yp5soTJoDzzm8SYM05R1FpCNd526Vnv/3a8B4JBog0K2gf716L8ZKGNU9RgVbUFbYAPzbQAo44nw0AgbwAYqsgGgXFFj4F3M9y7QDu22YgNAGSjjJWED2EBFNgCUK2qMrfT01AOvFRuYbwNA2YOyGa/srnPhHZ8vtDVSO+Y4tQ6EPW/9VzO0LbMw0Kp52tEXiTUtrqfD8jZcVaeV7Yuy3X77XgHKz+KDN3Hkg3ieYJipccqrGKOaFBKb+DGuoW9y55F2HeVGrfQ2TUc9q89OHmHnkXF2Mk1j0rw3vYpD+fXHR+HO3pOfD4fxYL4olCd0Dn1DyS3dmZsoEcx4CydUBBNO/Ekg4Yw+nVbPCw/SaMtu1q/Y/XBS2zrFZ+SloeCtdZGcZh2f+t2WIVW2RW2RLnObL+kTR67UBopDuXdTqO2hHsXTb+NupBSU+9OsncWIpPjt1Gh/SrVd2lNdf2c7j3RtEaxjMcZYQwgbwEe3k8qCdwDaY8rCOQB2QzZwfSg/f1jFU7ZQ9SDbW5BIbtfkrh1xzzuPuJ3gDCh7ixnJ69l5pOvkXG15jy7TbODKUJ6+T1/WU3YhzM4j2nsKwyFRj2ImlNl5BA81ak/TIAS0fb2uCuXnjwdxeHwSrxMaNgtlb9SEDlNYz1mHL4KQRi/fpUt3JtI3IxGyy1Oqn/jhdlBBvFmV1/25r4837QpyjWjaZT1NQ6t0h9aYWAJldh4BKj5U0GOZHleD8tQHfLahbxLKdmcRtdlprMEMfIvuPOKWYyaUv9urzVLZecTVkvf2XuV1ni1cB8oqjjz+4Z7buFkou+GLcP3k8HPPS5YCXshTrnTnkU7XHJSNNx564JEHe95oDFffgRBKdnlPNx3eEy65AxsoD2U12uLgDYvr4DDcs2ShzM4j4vgQhkDOYhh6OSinhvMF14SjMZybxx/GF7ZxLEQTnsPnKfcI5962vRSHsg5b+LuOHA7jveYslI9HcfxejrAIR1nYIXGJmK96KGivc15NjHq7O49osNrJH/a1NwkkGRNn5xEAeNsArLH9ikN5qQh5KLPzSErfvLeau7HM9OyZG5tmvfSBsEaqLnyfay+O3bp9AGXnZ3aRxoyOsihzI7HzSBmdi9hRabslv2Lx/M1AmRsB4GAD2MAWbAAo4wEU8wC2cMNQB8B/aRsAykAZKGMD2EBFNgCUK2qMS/fApI+Xhw3UbwNAGSjjJWED2EBFNgCUvcZg55HVPCkztrlh5xGA591j9Xuqq90DM+tdHspmRt+h3X1k/MQRKVZqnPIqQt7jziNqCc5GqIkjZtW3SVra6dbfuUuiOjeeHAIYTtFujVWPgc4u1NSe66TJd4B+wzZQHsrPT96C9lMXJroolBc19K3uPLITdkGh5NoVCV3U+RLG3trKITzdVe3CY2dxVuO2E0BP5Dup0yANAH5jNlAeyqFA7Dwi3Nl24Qy48PPZmwUn143wl+V009LwSkMxhLCGbAfp0fDLQnkIvOnyjc4/tCk+A+IbtoGrQ1mtqbzCHn3sPKK90D6UUwv+BAsKSbC+34v9d43orX0xZOBDUPY6kr633Ot4hvLjONDdsA1cCcrujtbjN02VnlMqfKGg7C7dyc4j+sZNAlFDWQNYLiwkQwh6gaHVoTywnnS/I+mDG68ZTe7FBq4E5c7AVEz54/Ponj8LZXYe6T9US+480kH55Yed8Y677ybdAEOesoFyb5dt4+0oKCdGaUwqx4a9J3TomLF1La4O5fNZe80fnseJfpNQHvAUz/Z40Z1HTPji/V7s3tvV9fCUt37DU79xnLmmTtuCshu+CHcaCT9Hvar72nmk92AvulB9YucRV78hTzkZQtE3CDHl+kFxTUjdW96Fofwqnj76G6Wu+6CvW8T+dGxEk4O0C079KsYAAAo8SURBVJX2/VIoN6LdqLVNU99wybhpD1jBg7lgyJgCWDvuNxx9ER/JkIReANNwNEZ7M/TKGEAkSKe9zmiQrLs6HtQ30C1Mi8+B9ug1OvR5K7ZTGMpnoSHs7jyy4oM+dh4x436DLaEyUJXxZLvjSJOaAKKub0RvkoeCsZl48l1qAkoeunlgA6BbAQnlXM9Wi0N5aeONjymvJ9LSMrfXB15v+30Bb2c+/LT33bDzyOY8spL2R17jeQSUCwDRM0gF5sCTLVQGBeYm4vGm8jdlnQVk412nrlUhlWDii6dTqkx8T+ewcRvYDJS5ocf3xGiFVthAvTYAlDfe63Lz1Xvz0Ta0TcwGgDJQ5ucwNoANVGQDQLmixoj1mnyHN4UN3JcNAGWgjJeEDWADFdkAUPYag51HVvPK7OiLxJoWevTFdUahrFZHz3buy5tDw8u191WhrBe4P4ix615IQ0iNU17FSO5x55HzWejp1o3Y/fAy3WNi55HpmgFzNMvYwBWhLBciehSPjxVBOSPUMPRvdecRCeOTOP51OpQ1zNl5ZNg2LudVkff2tL0alOV06w/Pr+JpJSir9ZS9adbdOhjKcJUXbNYM3jeiMf92rQq7SL78fvfR9xh12ieh1tNQ1/lpd9936TbeMqJnkVx/4ny+2s4j3Q1tVoyb4ynLjmxg7Yv8lk/x9Tq6sm3vpqNutGnOBq4D5ecP4qDWUF4ZyvtuQaDeovc2NLF394N7ES+/uAaSWZAol7bysHOe8hB4zPoQi5bu1PXoT6fOrz2hjePCUM6svSHzz3VYOePlmGu7vN+KPVwByjps8fSbNKKVoeyuCifXKN47Hq2BsvWM4w2YgbKbdut1uzdCkJ8XChkBxuj0aw1zfyEg9zuZrrtHn87Hm9o8AEStw4WhbNeLfnL16t73O5LuWLydOI4u27WB4lCWYYvHH19NoH9lKHshAwnJznM+K5A6kPagaRs4A2U37alQNiMRfLjaPM1rFMoxmLtet4Fw04jG/rfLerrpuuAO8lU6lIEyO4/EtOc7OhjfBgpD2d2bz12+8yAOj/46y6mGSo2+sHHf7rqKoDzgKV5n5xHXEMpAOdUp4Sm7bcH77h6+Ty0KQzkUeWVPuRdicDzja3rKZ9e7DTVY8UGf8rbdmLnMK+Zth2XIQdl446EH7v7SGHrQNxBCIaYctgef7xnM24KyE0Nm55Huxh6GXg7KZ3EegOrQ6Iu8Jzym0+jqcs83K3W/DzvYFpS9IXGBx5j1lHWoww6Ta19NHLkXGonGlM/i/GnfDrXztqJSXmUMPsYLDRaQVxBzHuApqNqYsTdDTl4fxItjsekkVDWMu51H9O4h+38Exp+KibPzCJMg3F9MvF/FHq4M5eDmH9Go42PK09O+uCcSDS+UKWfeW82VQYdevBEdI9rJapn10pOdRa48HLPa8rpNWwDKEwCzyk0Q82QLlUF74Ow8sko7FmozyrpN8ObadTNQzlWSY/dn2LQ5bX6rNgCU8XhWiYPd6g1AuYF3bTYAlIEyUMYGsIGKbAAoV9QYtfXYlAcvEhsobwNAGSjjJWED2EBFNgCUK2oMvJLyXgmao3ltNgCUgTJeEjaADVRkA0C5osaorcemPHiR2EB5G8hC+cuXL9X1oKkZfRhPeeNBczTHBta3gSyUv379KiSYP3/+zD8aYAPYADZQwAayUBb8oQAKoAAKFFUAKBeVm8xQAAVQIK8AUM7rw1EUQAEUKKoAUC4qN5mhAAqgQF4BoJzXh6MogAIoUFQBoFxUbjJDARRAgbwCQDmvD0dRAAVQoKgCQLmo3GSGAiiAAnkFgHJeH46iAAqgQFEFgHJRuckMBVAABfIKAOW8PhxFARRAgaIKLIDyT+Jt04hG/T+Ix1+LlpvMUAAFUGCTCsyE8u/i8ZtGPLz/XYvy97eiad6KnzYpEZVCARRAgXIKzINyCGH12YF0ufKTEwqgAApsSoFZUP79/YNovnkU2k+WYYy34u23jWi+xVfelHVQGRRAgeIKzIeyAfBP3zbi7d+FkK9AuXj7kSEKoMDGFFgG5V8fxYOBs/Ke8ZQ3Zh5UBwVQoLQC86H8zVvx9pvu4R6ecummIz8UQIEtKjALyqL3YC8YjbFFpagTCqAAChRQYB6UhR6jLGPJ6i8cjVGg4GSBAiiAAltUYCaUhRAyntxOHtEP+7YoEHVCARRAgZIKzIdyyVKSFwqgAArciQJA+U4ammqiAArchgJA+TbaiVKiAArciQJA+U4ammqiAArchgJA+TbaiVKiAArciQJA+U4ammqiAArchgJA+TbaiVKiAArciQJA+U4ammqiAArchgLLoGymW3fLeN5GpSklCqAACtSqwEwo67UuJIx/8tZWrrWalAsFUAAFbkOBmVDuKucveN99zzsUQAEUQIHpCgDl6ZpxBQqgAApcTAGgfDFpSRgFUAAFpisAlKdrxhUogAIocDEFgPLFpCVhFEABFJiuAFCerhlXoAAKoMDFFADKF5OWhFEABVBgugKzoayGwjk7jzTyPbtZT28BrkABFEABR4HZUHbS4C0KoAAKoMBKCgDllYQkGRRAARRYQwGgvIaKpIECKIACKykAlFcSkmRQAAVQYA0FgPIaKpIGCqAACqykAFBeSUiSQQEUQIE1FADKa6hIGiiAAiiwkgJAeSUhSQYFUAAF1lBgGZTZeWSNNiANFEABFGgVmAlldh5pFeQNCqAACqyowEwodyVg55FOC96hAAqgwFIFgPJSBbkeBVAABVZUACivKCZJoQAKoMBSBYDyUgW5HgVQAAVWVAAorygmSaEACqDAUgWA8lIFuR4FUAAFVlQAKK8oJkmhAAqgwFIFZkOZnUeWSs/1KIACKNBXYDaU+0nxDQqgAAqgwFIFgPJSBbkeBVAABVZUACivKCZJoQAKoMBSBYDyUgW5HgVQAAVWVAAorygmSaEACqDAUgWA8lIFuR4FUAAFVlQAKK8oJkmhAAqgwFIFgPJSBbkeBVAABVZUYAGUfxJvm0Y06v9BPP66YqlICgVQAAXuVIGZUNY7jzy8/13LpraFeit+ulMRqTYKoAAKrKXAPCiHEDZ79bWQXqt0pIMCKIACd6bALCj7W0DJMMZb8fbbRjTf4ivfmf1QXRRAgZUVmA9lA+Cfvm3E278LIV+B8sqtQ3IogAJ3p8AyKP/6KB4MnJX3jKd8dwZEhVEABdZVYD6Uv3kr3n7TPdzDU163YUgNBVDgPhWYBWXRe7AXjMa4Ty2pNQqgAAosVmAelIUeoyxjyeovHI2xuFgkgAIogAL3qcBMKAshZDy5nTyiH/bdp4TUGgVQAAXWU2A+lNcrAymhAAqgAAoYBYAypoACKIACFSkAlCtqDIqCAiiAAkAZG0ABFECBihQAyhU1BkVBARRAAaCMDaAACqBARQoA5Yoag6KgAAqgAFDGBlAABVCgIgWAckWNQVFQAAVQAChjAyiAAihQkQJAuaLGoCgogAIoAJSxARRAARSoSIH/B9E0wfsfWzTTAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이거 푼거랑 똑같은 논리로 접근 하면 돼!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XOR():\n",
    "    def __init__(self):\n",
    "        np.random.seed(1)\n",
    "        \n",
    "        #2층 신경망의 매개변수\n",
    "        def weight_init():\n",
    "            params = {}\n",
    "            params['W1'] = np.random.randn(2)\n",
    "            params['b1'] = np.random.rand(2)\n",
    "            params['W2'] = np.random.randn(2)\n",
    "            params['b2'] = np.random.rand(1)\n",
    "            return params\n",
    "        \n",
    "        self.params = weight_init()\n",
    "        \n",
    "    def predict(self,x):\n",
    "        W1,W2 = self.params['W1'].reshape(-1,1), self.params['W2'].reshape(-1,1)\n",
    "        b1,b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        A1 = np.dot(x,W1) + b1\n",
    "        Z1 = sigmoid(A1)\n",
    "        A2 = np.dot(Z1,W2) + b2\n",
    "        pred_y = sigmoid(A2)\n",
    "        \n",
    "        return pred_y\n",
    "    \n",
    "    def loss(self,x,true_y):\n",
    "        pred_y = self.predict(x)\n",
    "        return cross_entropy_error_for_bin(pred_y, true_y)\n",
    "    \n",
    "    def get_gradient(self,x,t):\n",
    "        def loss_grad(grad):\n",
    "            return self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = differential(loss_grad, self.params['W1'])\n",
    "        grads['b1'] = differential(loss_grad, self.params['b1'])\n",
    "        grads['W2'] = differential(loss_grad, self.params['W2'])\n",
    "        grads['b2'] = differential(loss_grad, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs : 100, Cost : 1.3735671088815753, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 200, Cost : 1.3651207639926013, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 300, Cost : 1.353442133114538, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 400, Cost : 1.3368594082199925, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 500, Cost : 1.3137221263821095, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 600, Cost : 1.2822263227992128, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 700, Cost : 1.2366196689828508, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 800, Cost : 1.1394214854008515, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 900, Cost : 0.888854116541407, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 1000, Cost : 0.6256077807762299, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n"
     ]
    }
   ],
   "source": [
    "XOR = XOR()\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y4 = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "train_loss_list5 = list()\n",
    "\n",
    "for i in range(epochs):\n",
    "    grads = XOR.get_gradient(X,Y4)\n",
    "    \n",
    "    for key in ['W1','b1','W2','b2']:\n",
    "        XOR.params[key] -= lr*grads[key]\n",
    "        \n",
    "    loss = XOR.loss(X,Y4)\n",
    "    train_loss_list5.append(loss)\n",
    "    \n",
    "    if i%100 ==99:\n",
    "        print('Epochs : {}, Cost : {}, Weights : {}, Bias : {}'.format(i+1, loss, AND.weights, AND.bias))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2609618 ],\n",
       "       [0.7639633 ],\n",
       "       [0.68281861],\n",
       "       [0.25773454]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XOR.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 학습률 조정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs : 100, Cost : 0.26423816180460497, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 200, Cost : 0.15472193951681867, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 300, Cost : 0.10712110936785606, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 400, Cost : 0.08124302351322064, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 500, Cost : 0.0651692042852715, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 600, Cost : 0.05428045337331503, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 700, Cost : 0.04644351003527885, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 800, Cost : 0.040545985198490495, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 900, Cost : 0.035953989738723505, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n",
      "Epochs : 1000, Cost : 0.03228103535893161, Weights : [4.27548114 4.27284863], Bias : [-6.6027234]\n"
     ]
    }
   ],
   "source": [
    "lr = 0.3\n",
    "\n",
    "for i in range(epochs):\n",
    "    grads = XOR.get_gradient(X,Y4)\n",
    "    \n",
    "    for key in ['W1','b1','W2','b2']:\n",
    "        XOR.params[key] -= lr*grads[key]\n",
    "        \n",
    "    loss = XOR.loss(X,Y4)\n",
    "    train_loss_list5.append(loss)\n",
    "    \n",
    "    if i%100 ==99:\n",
    "        print('Epochs : {}, Cost : {}, Weights : {}, Bias : {}'.format(i+1, loss, AND.weights, AND.bias))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01437328],\n",
       "       [0.97911594],\n",
       "       [0.98563516],\n",
       "       [0.0144055 ]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XOR.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.3 MNIST 2층 신경망 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지난번엔 우리가 '이미 학습된 데이터(pre-trained data)'를 가져다가 MNIST를 학습을 시켰었잖아<br>\n",
    "오늘은 직접! 훈련까지 시켜볼거야"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습의 순서는 다음과 같아<br>\n",
    "1. 미니배치\n",
    "2. 기울기 산출\n",
    "3. 매개변수 갱신\n",
    "4. 1~3 반복\n",
    "\n",
    "훈련데이터중 일부를 뽑아온 애를 미니배치라 했어. 얘의 손실함수를 줄이는게 목표야 (전체의 손실함수를 한번에 다 줄이려는게 아니야!)\n",
    "이때, '일부'를 뽑는다고 했잖아? 그리고 얘한테 '경사하강법'을 적용할꺼잖아?<br>\n",
    "그래서 얘를, <b>'확률적 경사 하강법'(Stochastic Gradient Descent)</b> 라고 해"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#나도 힘들어. 그냥 불러서 쓸거야\n",
    "import os, sys\n",
    "os.getcwd()\n",
    "sys.path.append('D:\\python_projects\\python_practice\\deep_scratch_01\\deep-learning-from-scratch-master')\n",
    "\n",
    "#\n",
    "# deep-learning-from-scratch-master라는 폴더를 찾아\n",
    "# 경로를 복사해\n",
    "# sys.path.append 뒤에 붙여넣으렴\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.mnist import load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = load_mnist(normalize = True, flatten = True, one_hot_label = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼 파라미터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 0.1\n",
    "batch_size = 100\n",
    "train_size = x_train.shape[0]\n",
    "print(train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용할 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#위에있는거랑 겹치는 부분이 많아\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def mean_squared_error(pred_y,true_y):\n",
    "    return 0.5 * np.sum((true_y - pred_y)**2 )\n",
    "\n",
    "def cross_entropy_error(pred_y, true_y):\n",
    "    if true_y.ndim==1:\n",
    "        true_y = true_y.reshape(1,-1)\n",
    "        pred_y = pred_y.reshape(1,-1)\n",
    "        \n",
    "    delta = 1e-7\n",
    "    return -np.sum(true_y * np.log(pred_y + delta))\n",
    "\n",
    "def cross_entropy_error_for_batch(pred_y,true_y):\n",
    "    if true_y.ndim ==1:\n",
    "        true_y = true_y.reshape(1,-1)\n",
    "        pred_y = pred_y.reshape(1,-1)\n",
    "  \n",
    "    delta = 1e-7\n",
    "    batch_size = pred_y.shape[0]\n",
    "    return -np.sum(true_y * np.log(pred_y + delta)) /batch_size\n",
    "\n",
    "def cross_entropy_error_for_bin(pred_y,true_y):\n",
    "    return 0.5 * np.sum((-true_y * np.log(pred_y) - (1 - true_y) * np.log(1-pred_y)))\n",
    "\n",
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y\n",
    "\n",
    "def numerical_gradient_1d(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "       \n",
    "    return grad\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim ==1:\n",
    "        return numerical_gradient_1d(f,X)\n",
    "    \n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = numerical_gradient_1d(f,x)\n",
    "            \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_layer():\n",
    "    def __init__(self):\n",
    "        \n",
    "        def weight_init(input,hidden,output):\n",
    "            np.random.seed(1)\n",
    "            params = {}\n",
    "            params['W1'] = 0.01 * np.random.randn(input,hidden)\n",
    "            params['b1'] = np.zeros(hidden)\n",
    "            params['W2'] = 0.01 * np.random.randn(hidden,output)\n",
    "            params['b2'] = np.zeros(output)\n",
    "            \n",
    "            return params\n",
    "        \n",
    "        self.params = weight_init(784,64,10)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        W1,W2 = self.params['W1'],self.params['W2']\n",
    "        b1,b2 = self.params['b1'],self.params['b2']\n",
    "        \n",
    "        A1 = np.dot(x,W1) + b1\n",
    "        Z1 = sigmoid(A1)\n",
    "        A2 = np.dot(Z1,W2) + b2\n",
    "        pred_y = softmax(A2)\n",
    "        \n",
    "        return pred_y\n",
    "    \n",
    "    def loss(self,x,true_y):\n",
    "        pred_y = self.predict(x)\n",
    "        return cross_entropy_error_for_bin(pred_y,true_y)\n",
    "    \n",
    "    def accuracy(self,x,true_y):\n",
    "        pred_y = self.predict(x)\n",
    "        y_argmax = np.argmax(pred_y, axis = 1)\n",
    "        t_argmax = np.argmax(true_y, axis = 1)\n",
    "        \n",
    "        accuracy = np.sum(y_argmax == t_argmax) / float(x.shape[0])\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def get_gradient(self,x,t):\n",
    "        def loss_grad(grad):\n",
    "            return self.loss(x,t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_grad,self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_grad,self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_grad,self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_grad,self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40558dd48dc3452880d2135e32647b4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, Cost : 402.8164838421469, Train Accuracy : 0.09871666666666666, Test Accuracy : 0.098\n",
      "Epoch : 2, Cost : 371.57567905511667, Train Accuracy : 0.09736666666666667, Test Accuracy : 0.0982\n",
      "Epoch : 3, Cost : 354.12208413052724, Train Accuracy : 0.10126666666666667, Test Accuracy : 0.1027\n",
      "Epoch : 4, Cost : 352.2199661695997, Train Accuracy : 0.10221666666666666, Test Accuracy : 0.101\n",
      "Epoch : 5, Cost : 346.23202071921105, Train Accuracy : 0.10411666666666666, Test Accuracy : 0.1027\n",
      "Epoch : 6, Cost : 347.68277491597735, Train Accuracy : 0.09946666666666666, Test Accuracy : 0.0988\n",
      "Epoch : 7, Cost : 352.57694522799136, Train Accuracy : 0.09765, Test Accuracy : 0.1005\n",
      "Epoch : 8, Cost : 348.12490008726587, Train Accuracy : 0.10136666666666666, Test Accuracy : 0.1035\n",
      "Epoch : 9, Cost : 345.65574279364773, Train Accuracy : 0.10428333333333334, Test Accuracy : 0.1\n",
      "Epoch : 10, Cost : 340.8321878450173, Train Accuracy : 0.09751666666666667, Test Accuracy : 0.0974\n",
      "Epoch : 11, Cost : 342.74467819267335, Train Accuracy : 0.12783333333333333, Test Accuracy : 0.1224\n",
      "Epoch : 12, Cost : 343.9496917773555, Train Accuracy : 0.12223333333333333, Test Accuracy : 0.1154\n",
      "Epoch : 13, Cost : 343.4104929948221, Train Accuracy : 0.12613333333333332, Test Accuracy : 0.1205\n",
      "Epoch : 14, Cost : 342.89245084518984, Train Accuracy : 0.09648333333333334, Test Accuracy : 0.0952\n",
      "Epoch : 15, Cost : 342.79052162069513, Train Accuracy : 0.1441, Test Accuracy : 0.1421\n",
      "Epoch : 16, Cost : 342.602536699634, Train Accuracy : 0.19, Test Accuracy : 0.1914\n",
      "Epoch : 17, Cost : 340.04582008198554, Train Accuracy : 0.10151666666666667, Test Accuracy : 0.1024\n",
      "Epoch : 18, Cost : 342.0034656788959, Train Accuracy : 0.10228333333333334, Test Accuracy : 0.1043\n",
      "Epoch : 19, Cost : 342.9609071925671, Train Accuracy : 0.16605, Test Accuracy : 0.1609\n",
      "Epoch : 20, Cost : 341.28829211871926, Train Accuracy : 0.18458333333333332, Test Accuracy : 0.1886\n",
      "Epoch : 21, Cost : 341.40703688357087, Train Accuracy : 0.13885, Test Accuracy : 0.1335\n",
      "Epoch : 22, Cost : 339.7341535537558, Train Accuracy : 0.13973333333333332, Test Accuracy : 0.1364\n",
      "Epoch : 23, Cost : 341.71903586085494, Train Accuracy : 0.1922, Test Accuracy : 0.1895\n",
      "Epoch : 24, Cost : 339.9430966088912, Train Accuracy : 0.2629166666666667, Test Accuracy : 0.262\n",
      "Epoch : 25, Cost : 337.7477130928904, Train Accuracy : 0.17113333333333333, Test Accuracy : 0.1696\n",
      "Epoch : 26, Cost : 336.28215617769877, Train Accuracy : 0.21365, Test Accuracy : 0.2129\n",
      "Epoch : 27, Cost : 334.9857098486955, Train Accuracy : 0.20678333333333335, Test Accuracy : 0.2081\n",
      "Epoch : 28, Cost : 332.03466409549384, Train Accuracy : 0.2762833333333333, Test Accuracy : 0.2784\n",
      "Epoch : 29, Cost : 328.19425899496, Train Accuracy : 0.26131666666666664, Test Accuracy : 0.2677\n",
      "Epoch : 30, Cost : 324.2417628975337, Train Accuracy : 0.37016666666666664, Test Accuracy : 0.3757\n",
      "Epoch : 31, Cost : 324.04389907176835, Train Accuracy : 0.23406666666666667, Test Accuracy : 0.2348\n",
      "Epoch : 32, Cost : 331.57224133183763, Train Accuracy : 0.27535, Test Accuracy : 0.2797\n",
      "Epoch : 33, Cost : 325.8734563742794, Train Accuracy : 0.25971666666666665, Test Accuracy : 0.2516\n",
      "Epoch : 34, Cost : 324.9264757661573, Train Accuracy : 0.359, Test Accuracy : 0.3624\n",
      "Epoch : 35, Cost : 323.4432111688303, Train Accuracy : 0.31035, Test Accuracy : 0.3063\n",
      "Epoch : 36, Cost : 330.6691931539797, Train Accuracy : 0.29115, Test Accuracy : 0.2895\n",
      "Epoch : 37, Cost : 344.59674540387454, Train Accuracy : 0.13435, Test Accuracy : 0.1234\n",
      "Epoch : 38, Cost : 317.3082656777031, Train Accuracy : 0.36365, Test Accuracy : 0.3514\n",
      "Epoch : 39, Cost : 310.66987471184905, Train Accuracy : 0.4029, Test Accuracy : 0.399\n",
      "Epoch : 40, Cost : 311.6696720109154, Train Accuracy : 0.4270833333333333, Test Accuracy : 0.4286\n",
      "Epoch : 41, Cost : 312.76943889827237, Train Accuracy : 0.39815, Test Accuracy : 0.3957\n",
      "Epoch : 42, Cost : 321.92266277410545, Train Accuracy : 0.30268333333333336, Test Accuracy : 0.2901\n",
      "Epoch : 43, Cost : 323.5431457572282, Train Accuracy : 0.3641666666666667, Test Accuracy : 0.3624\n",
      "Epoch : 44, Cost : 312.4161503572003, Train Accuracy : 0.38863333333333333, Test Accuracy : 0.386\n",
      "Epoch : 45, Cost : 308.19656929983273, Train Accuracy : 0.34013333333333334, Test Accuracy : 0.3398\n",
      "Epoch : 46, Cost : 301.59389534766694, Train Accuracy : 0.461, Test Accuracy : 0.4568\n",
      "Epoch : 47, Cost : 302.324444741408, Train Accuracy : 0.4648, Test Accuracy : 0.4599\n",
      "Epoch : 48, Cost : 317.9536589660647, Train Accuracy : 0.3547666666666667, Test Accuracy : 0.3411\n",
      "Epoch : 49, Cost : 300.32566660481245, Train Accuracy : 0.41618333333333335, Test Accuracy : 0.4109\n",
      "Epoch : 50, Cost : 310.7488257248816, Train Accuracy : 0.38705, Test Accuracy : 0.3911\n",
      "Epoch : 51, Cost : 318.2279889844905, Train Accuracy : 0.30196666666666666, Test Accuracy : 0.2912\n",
      "Epoch : 52, Cost : 301.56744356779643, Train Accuracy : 0.49106666666666665, Test Accuracy : 0.4852\n",
      "Epoch : 53, Cost : 301.13840674608946, Train Accuracy : 0.4094, Test Accuracy : 0.4051\n",
      "Epoch : 54, Cost : 332.0357879862454, Train Accuracy : 0.3515666666666667, Test Accuracy : 0.3452\n",
      "Epoch : 55, Cost : 318.99205492039926, Train Accuracy : 0.4508666666666667, Test Accuracy : 0.4493\n",
      "Epoch : 56, Cost : 322.2311509075726, Train Accuracy : 0.339, Test Accuracy : 0.3318\n",
      "Epoch : 57, Cost : 319.3729683486307, Train Accuracy : 0.3318, Test Accuracy : 0.3254\n",
      "Epoch : 58, Cost : 304.4837997666215, Train Accuracy : 0.47131666666666666, Test Accuracy : 0.4674\n",
      "Epoch : 59, Cost : 297.8159765231569, Train Accuracy : 0.39591666666666664, Test Accuracy : 0.3924\n",
      "Epoch : 60, Cost : 305.261520463841, Train Accuracy : 0.5043666666666666, Test Accuracy : 0.5017\n",
      "Epoch : 61, Cost : 298.510113008722, Train Accuracy : 0.5098166666666667, Test Accuracy : 0.5031\n",
      "Epoch : 62, Cost : 287.9598690405269, Train Accuracy : 0.489, Test Accuracy : 0.4852\n",
      "Epoch : 63, Cost : 297.1971165348923, Train Accuracy : 0.5226666666666666, Test Accuracy : 0.5134\n",
      "Epoch : 64, Cost : 295.6805204587012, Train Accuracy : 0.5359333333333334, Test Accuracy : 0.5379\n",
      "Epoch : 65, Cost : 295.3546414283456, Train Accuracy : 0.48401666666666665, Test Accuracy : 0.484\n",
      "Epoch : 66, Cost : 304.6919885510263, Train Accuracy : 0.5070666666666667, Test Accuracy : 0.5083\n",
      "Epoch : 67, Cost : 305.8681000920927, Train Accuracy : 0.42461666666666664, Test Accuracy : 0.429\n",
      "Epoch : 68, Cost : 302.52449442428417, Train Accuracy : 0.47923333333333334, Test Accuracy : 0.4755\n",
      "Epoch : 69, Cost : 292.9147007598118, Train Accuracy : 0.48255, Test Accuracy : 0.4848\n",
      "Epoch : 70, Cost : 295.9157937512478, Train Accuracy : 0.5189, Test Accuracy : 0.5296\n",
      "Epoch : 71, Cost : 293.29388944366923, Train Accuracy : 0.5212166666666667, Test Accuracy : 0.5233\n",
      "Epoch : 72, Cost : 300.2937802478701, Train Accuracy : 0.41963333333333336, Test Accuracy : 0.4203\n",
      "Epoch : 73, Cost : 314.42472993969045, Train Accuracy : 0.3601666666666667, Test Accuracy : 0.3609\n",
      "Epoch : 74, Cost : 308.109368828943, Train Accuracy : 0.44788333333333336, Test Accuracy : 0.4393\n",
      "Epoch : 75, Cost : 292.57738055847085, Train Accuracy : 0.5543666666666667, Test Accuracy : 0.5549\n",
      "Epoch : 76, Cost : 290.2166337736786, Train Accuracy : 0.5485166666666667, Test Accuracy : 0.5469\n",
      "Epoch : 77, Cost : 284.97625714945707, Train Accuracy : 0.5880833333333333, Test Accuracy : 0.5896\n",
      "Epoch : 78, Cost : 286.8066192036675, Train Accuracy : 0.6146833333333334, Test Accuracy : 0.6255\n",
      "Epoch : 79, Cost : 280.8704377892342, Train Accuracy : 0.6181166666666666, Test Accuracy : 0.6154\n",
      "Epoch : 80, Cost : 280.3725417827603, Train Accuracy : 0.6022666666666666, Test Accuracy : 0.6023\n",
      "Epoch : 81, Cost : 294.3658912116043, Train Accuracy : 0.54235, Test Accuracy : 0.548\n",
      "Epoch : 82, Cost : 286.7656006314743, Train Accuracy : 0.5644166666666667, Test Accuracy : 0.5628\n",
      "Epoch : 83, Cost : 304.8415632826875, Train Accuracy : 0.5647166666666666, Test Accuracy : 0.571\n",
      "Epoch : 84, Cost : 309.07844260503146, Train Accuracy : 0.5349833333333334, Test Accuracy : 0.5409\n",
      "Epoch : 85, Cost : 302.7095223722164, Train Accuracy : 0.4614, Test Accuracy : 0.4498\n",
      "Epoch : 86, Cost : 308.39094820287954, Train Accuracy : 0.5588833333333333, Test Accuracy : 0.5547\n",
      "Epoch : 87, Cost : 287.43023693432156, Train Accuracy : 0.5220166666666667, Test Accuracy : 0.524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 88, Cost : 288.7753458727649, Train Accuracy : 0.5432, Test Accuracy : 0.5437\n",
      "Epoch : 89, Cost : 302.61241215699783, Train Accuracy : 0.5399, Test Accuracy : 0.5408\n",
      "Epoch : 90, Cost : 283.96032104154955, Train Accuracy : 0.6251666666666666, Test Accuracy : 0.6211\n",
      "Epoch : 91, Cost : 284.87820066264374, Train Accuracy : 0.66715, Test Accuracy : 0.6617\n",
      "Epoch : 92, Cost : 276.0302408935228, Train Accuracy : 0.68755, Test Accuracy : 0.6888\n",
      "Epoch : 93, Cost : 282.76912897609685, Train Accuracy : 0.7214833333333334, Test Accuracy : 0.7189\n",
      "Epoch : 94, Cost : 270.8816176076607, Train Accuracy : 0.74785, Test Accuracy : 0.7544\n",
      "Epoch : 95, Cost : 275.7640755414362, Train Accuracy : 0.7497333333333334, Test Accuracy : 0.7441\n",
      "Epoch : 96, Cost : 267.16212811279706, Train Accuracy : 0.76365, Test Accuracy : 0.7617\n",
      "Epoch : 97, Cost : 274.52023678670804, Train Accuracy : 0.7708, Test Accuracy : 0.7675\n",
      "Epoch : 98, Cost : 293.99088361624274, Train Accuracy : 0.5806833333333333, Test Accuracy : 0.5873\n",
      "Epoch : 99, Cost : 297.8953851119378, Train Accuracy : 0.5867833333333333, Test Accuracy : 0.5799\n",
      "Epoch : 100, Cost : 283.6699644991021, Train Accuracy : 0.6155166666666667, Test Accuracy : 0.6215\n",
      "\n",
      "learning time : 4669.237923s\n"
     ]
    }
   ],
   "source": [
    "model = two_layer()\n",
    "\n",
    "train_loss_list = list()\n",
    "train_acc_list = list()\n",
    "test_acc_list = list()\n",
    "iter_per_epoch = max(train_size/batch_size,1)\n",
    "\n",
    "start_time = time.time()\n",
    "for i in tqdm(range(epochs)):\n",
    "    \n",
    "    batch_idx = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_idx]\n",
    "    y_batch = y_train[batch_idx]\n",
    "    \n",
    "    grads = model.get_gradient(x_batch,y_batch)\n",
    "    \n",
    "    for key in grads.keys():\n",
    "        model.params[key] -= lr * grads[key]\n",
    "        \n",
    "    loss = model.loss(x_batch,y_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    train_accuracy = model.accuracy(x_train,y_train)\n",
    "    test_accuracy = model.accuracy(x_test,y_test)\n",
    "    train_acc_list.append(train_accuracy)\n",
    "    test_acc_list.append(test_accuracy)\n",
    "    \n",
    "    print('Epoch : {}, Cost : {}, Train Accuracy : {}, Test Accuracy : {}'.format(i+1, loss, train_accuracy, test_accuracy))\n",
    "    \n",
    "end_time = time.time()\n",
    "print('learning time : {:3f}s'.format(end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
